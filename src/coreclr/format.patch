diff --git a/src/coreclr/jit/codegenxarch.cpp b/src/coreclr/jit/codegenxarch.cpp
index dab9a234c85..cad10d23a6f 100644
--- a/src/coreclr/jit/codegenxarch.cpp
+++ b/src/coreclr/jit/codegenxarch.cpp
@@ -7453,41 +7453,42 @@ void CodeGen::genFloatToIntCast(GenTree* treeNode)
     var_types srcType = op1->TypeGet();
     assert(varTypeIsFloating(srcType) && !varTypeIsFloating(dstType));
 
     // We should never be seeing dstType whose size is neither sizeof(TYP_INT) nor sizeof(TYP_LONG).
     // For conversions to byte/sbyte/int16/uint16 from float/double, we would expect the
     // front-end or lowering phase to have generated two levels of cast. The first one is
     // for float or double to int32/uint32 and the second one for narrowing int32/uint32 to
     // the required smaller int type.
     emitAttr dstSize = EA_ATTR(genTypeSize(dstType));
     noway_assert((dstSize == EA_ATTR(genTypeSize(TYP_INT))) || (dstSize == EA_ATTR(genTypeSize(TYP_LONG))));
 
     // We shouldn't be seeing uint64 here as it should have been converted
     // into a helper call by either front-end or lowering phase, unless we have AVX512F
     // accelerated conversions.
     assert(!varTypeIsUnsigned(dstType) || (dstSize != EA_ATTR(genTypeSize(TYP_LONG))) ||
            compiler->compOpportunisticallyDependsOn(InstructionSet_AVX512F));
 
     // If the dstType is TYP_UINT, we have 32-bits to encode the
     // float number. Any of 33rd or above bits can be the sign bit.
     // To achieve it we pretend as if we are converting it to a long.
-    if (varTypeIsUnsigned(dstType) && (dstSize == EA_ATTR(genTypeSize(TYP_INT))) && !compiler->compOpportunisticallyDependsOn(InstructionSet_AVX512F))
+    if (varTypeIsUnsigned(dstType) && (dstSize == EA_ATTR(genTypeSize(TYP_INT))) &&
+        !compiler->compOpportunisticallyDependsOn(InstructionSet_AVX512F))
     {
         dstType = TYP_LONG;
     }
 
     // Note that we need to specify dstType here so that it will determine
     // the size of destination integer register and also the rex.w prefix.
     genConsumeOperands(treeNode->AsOp());
     instruction ins = ins_FloatConv(dstType, srcType, emitTypeSize(srcType));
     GetEmitter()->emitInsBinary(ins, emitTypeSize(dstType), treeNode, op1);
     genProduceReg(treeNode);
 }
 
 //------------------------------------------------------------------------
 // genCkfinite: Generate code for ckfinite opcode.
 //
 // Arguments:
 //    treeNode - The GT_CKFINITE node
 //
 // Return Value:
 //    None.
diff --git a/src/coreclr/jit/hwintrinsicxarch.cpp b/src/coreclr/jit/hwintrinsicxarch.cpp
index 2e4d2480d1a..94526bbe620 100644
--- a/src/coreclr/jit/hwintrinsicxarch.cpp
+++ b/src/coreclr/jit/hwintrinsicxarch.cpp
@@ -1355,112 +1355,112 @@ GenTree* Compiler::impSpecialIntrinsic(NamedIntrinsic        intrinsic,
             assert(sig->numArgs == 3);
 
             op3 = impSIMDPopStack();
             op2 = impSIMDPopStack();
             op1 = impSIMDPopStack();
 
             retNode = gtNewSimdCndSelNode(retType, op1, op2, op3, simdBaseJitType, simdSize);
             break;
         }
 
         case NI_Vector128_ConvertToDouble:
         case NI_Vector256_ConvertToDouble:
         case NI_Vector512_ConvertToDouble:
         {
             if (IsBaselineVector512IsaSupportedOpportunistically())
             {
                 assert(sig->numArgs == 1);
                 assert(simdBaseType == TYP_LONG || simdBaseType == TYP_ULONG);
 
                 intrinsic = (simdSize == 16) ? NI_AVX512DQ_VL_ConvertToVector128Double
-                          : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256Double
-                                             : NI_AVX512DQ_VL_ConvertToVector512Double;
+                                             : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256Double
+                                                                : NI_AVX512DQ_VL_ConvertToVector512Double;
 
                 op1     = impSIMDPopStack();
                 retNode = gtNewSimdHWIntrinsicNode(retType, op1, intrinsic, simdBaseJitType, simdSize);
             }
 
             break;
         }
 
         case NI_Vector128_ConvertToInt64:
         case NI_Vector256_ConvertToInt64:
         case NI_Vector512_ConvertToInt64:
         {
             if (IsBaselineVector512IsaSupportedOpportunistically())
             {
                 assert(sig->numArgs == 1);
                 assert(simdBaseType == TYP_DOUBLE);
 
 #ifdef TARGET_XARCH
                 intrinsic = (simdSize == 16) ? NI_AVX512DQ_VL_ConvertToVector128Int64WithTruncation
-                          : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256Int64WithTruncation
-                                             : NI_AVX512DQ_VL_ConvertToVector512Int64WithTruncation;
+                                             : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256Int64WithTruncation
+                                                                : NI_AVX512DQ_VL_ConvertToVector512Int64WithTruncation;
 #else
                 intrinsic = (simdSize == 16) ? NI_AVX512DQ_VL_ConvertToVector128Int64
-                          : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256Int64
-                                             : NI_AVX512DQ_VL_ConvertToVector512Int64;
+                                             : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256Int64
+                                                                : NI_AVX512DQ_VL_ConvertToVector512Int64;
 #endif // TARGET_XARCH
 
                 op1     = impSIMDPopStack();
                 retNode = gtNewSimdHWIntrinsicNode(retType, op1, intrinsic, simdBaseJitType, simdSize);
             }
 
             break;
         }
 
         case NI_Vector128_ConvertToUInt32:
         case NI_Vector256_ConvertToUInt32:
         case NI_Vector512_ConvertToUInt32:
         {
             if (IsBaselineVector512IsaSupportedOpportunistically())
             {
                 assert(sig->numArgs == 1);
                 assert(simdBaseType == TYP_FLOAT);
 
-                intrinsic = (simdSize == 16) ? NI_AVX512F_VL_ConvertToVector128UInt32 
-                          : (simdSize == 32) ? NI_AVX512F_VL_ConvertToVector256UInt32
-                                             : NI_AVX512F_VL_ConvertToVector512UInt32;
+                intrinsic = (simdSize == 16) ? NI_AVX512F_VL_ConvertToVector128UInt32
+                                             : (simdSize == 32) ? NI_AVX512F_VL_ConvertToVector256UInt32
+                                                                : NI_AVX512F_VL_ConvertToVector512UInt32;
 
                 op1     = impSIMDPopStack();
                 retNode = gtNewSimdHWIntrinsicNode(retType, op1, intrinsic, simdBaseJitType, simdSize);
             }
             break;
         }
 
         case NI_Vector128_ConvertToUInt64:
         case NI_Vector256_ConvertToUInt64:
         case NI_Vector512_ConvertToUInt64:
         {
             if (IsBaselineVector512IsaSupportedOpportunistically())
             {
                 assert(sig->numArgs == 1);
                 assert(simdBaseType == TYP_DOUBLE);
 
 #ifdef TARGET_XARCH
                 intrinsic = (simdSize == 16) ? NI_AVX512DQ_VL_ConvertToVector128UInt64WithTruncation
-                          : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256UInt64WithTruncation
-                                             : NI_AVX512DQ_VL_ConvertToVector512UInt64WithTruncation;
+                                             : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256UInt64WithTruncation
+                                                                : NI_AVX512DQ_VL_ConvertToVector512UInt64WithTruncation;
 #else
                 intrinsic = (simdSize == 16) ? NI_AVX512DQ_VL_ConvertToVector128UInt64
-                          : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256UInt64
-                                             : NI_AVX512DQ_VL_ConvertToVector512UInt64;
+                                             : (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256UInt64
+                                                                : NI_AVX512DQ_VL_ConvertToVector512UInt64;
 #endif // TARGET_XARCH
                 op1     = impSIMDPopStack();
                 retNode = gtNewSimdHWIntrinsicNode(retType, op1, intrinsic, simdBaseJitType, simdSize);
             }
 
             break;
         }
 
         case NI_Vector128_ConvertToInt32:
         case NI_Vector256_ConvertToInt32:
         case NI_Vector512_ConvertToInt32:
         {
             assert(sig->numArgs == 1);
             assert(simdBaseType == TYP_FLOAT);
 
             switch (simdSize)
             {
                 case 16:
                     intrinsic = NI_SSE2_ConvertToVector128Int32WithTruncation;
                     break;
@@ -1493,41 +1493,42 @@ GenTree* Compiler::impSpecialIntrinsic(NamedIntrinsic        intrinsic,
                     case 16:
                         intrinsic = NI_SSE2_ConvertToVector128Single;
                         break;
                     case 32:
                         intrinsic = NI_AVX_ConvertToVector256Single;
                         break;
                     case 64:
                         intrinsic = NI_AVX512F_ConvertToVector512Single;
                         break;
                     default:
                         unreached();
                 }
 
                 op1     = impSIMDPopStack();
                 retNode = gtNewSimdHWIntrinsicNode(retType, op1, intrinsic, simdBaseJitType, simdSize);
             }
             else if (simdBaseType == TYP_UINT)
             {
                 if (IsBaselineVector512IsaSupportedOpportunistically())
                 {
-                    intrinsic = (simdSize == 32) ? NI_AVX512F_VL_ConvertToVector256Single : NI_AVX512F_VL_ConvertToVector128Single;
+                    intrinsic = (simdSize == 32) ? NI_AVX512F_VL_ConvertToVector256Single
+                                                 : NI_AVX512F_VL_ConvertToVector128Single;
 
                     op1     = impSIMDPopStack();
                     retNode = gtNewSimdHWIntrinsicNode(retType, op1, intrinsic, simdBaseJitType, simdSize);
                 }
             }
             else
             {
                 unreached();
             }
             break;
         }
 
         case NI_Vector128_Create:
         case NI_Vector256_Create:
         case NI_Vector512_Create:
         {
             if (sig->numArgs == 1)
             {
 #if defined(TARGET_X86)
                 if (varTypeIsLong(simdBaseType) && !impStackTop(0).val->IsIntegralConst())
diff --git a/src/coreclr/jit/instrsxarch.h b/src/coreclr/jit/instrsxarch.h
index 0959e87f524..add7a79abbd 100644
--- a/src/coreclr/jit/instrsxarch.h
+++ b/src/coreclr/jit/instrsxarch.h
@@ -616,51 +616,48 @@ INST3(knotw,            "knotw",            IUM_WR, BAD_CODE,               BAD_
 INST3(korw,             "korw",             IUM_WR, BAD_CODE,               BAD_CODE,     PCKFLT(0x45),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Bitwise logical OR masks
 INST3(kortestw,         "kortestw",         IUM_RD, BAD_CODE,               BAD_CODE,     PCKFLT(0x98),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX | Resets_OF      | Resets_SF     | Writes_ZF     | Resets_AF     | Resets_PF     | Writes_CF      | KInstruction)                  // OR masks and set flags
 INST3(kshiftlw,         "kshiftlw",         IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x32),                   INS_TT_NONE,                                          REX_W1                       | Encoding_VEX                                                                                                   | KInstruction)                  // Shift left mask registers
 INST3(kshiftrw,         "kshiftrw",         IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x30),                   INS_TT_NONE,                                          REX_W1                       | Encoding_VEX                                                                                                   | KInstruction)                  // Shift right mask registers
 INST3(kunpckbw,         "kunpckbw",         IUM_RD, BAD_CODE,               BAD_CODE,     PCKDBL(0x4B),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Unpack for mask registers
 INST3(kxnorw,           "kxnorw",           IUM_WR, BAD_CODE,               BAD_CODE,     PCKFLT(0x46),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Bitwise logical XNOR masks
 INST3(kxorw,            "kxorw",            IUM_WR, BAD_CODE,               BAD_CODE,     PCKFLT(0x47),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Bitwise logical XOR masks
 INST3(valignd,          "alignd",           IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x03),                   INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Align doubleword vectors
 INST3(valignq,          "alignq",           IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x03),                   INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Align quadword vectors
 INST3(vbroadcastf64x2,  "broadcastf64x2",   IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x1A),                   INS_TT_TUPLE2,                       Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Broadcast packed float values read from memory to entire register
 INST3(vbroadcasti64x2,  "broadcasti64x2",   IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x5A),                   INS_TT_TUPLE2,                       Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Broadcast packed integer values read from memory to entire register
 INST3(vbroadcastf64x4,  "broadcastf64x4",   IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x1B),                   INS_TT_TUPLE2,                       Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Broadcast packed float values read from memory to entire register
 INST3(vbroadcasti64x4,  "broadcasti64x4",   IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x5B),                   INS_TT_TUPLE2,                       Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Broadcast packed integer values read from memory to entire register
 INST3(vcmpps,           "cmpps",            IUM_WR, BAD_CODE,               BAD_CODE,     PCKFLT(0xC2),                  INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                          // compare packed singles
 INST3(vcmpss,           "cmpss",            IUM_WR, BAD_CODE,               BAD_CODE,     SSEFLT(0xC2),                  INS_TT_TUPLE1_SCALAR,                Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                          // compare scalar singles
 INST3(vcmppd,           "cmppd",            IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0xC2),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                          // compare packed doubles
 INST3(vcmpsd,           "cmpsd",            IUM_WR, BAD_CODE,               BAD_CODE,     SSEDBL(0xC2),                  INS_TT_TUPLE1_SCALAR,                Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                          // compare scalar doubles
 INST3(vcvtpd2udq,       "cvtpd2udq",        IUM_WR, BAD_CODE,               BAD_CODE,     PCKFLT(0x79),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt packed doubles to unsigned DWORDs
 INST3(vcvtps2udq,       "cvtps2udq",        IUM_WR, BAD_CODE,               BAD_CODE,     PCKFLT(0x79),                  INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // cvt packed singles to unsigned DWORDs
 INST3(vcvtsd2usi,       "cvtsd2usi",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEDBL(0x79),                  INS_TT_TUPLE1_FIXED,                 Input_64Bit    | REX_WX                       | Encoding_EVEX)                                                                                                                                  // cvt scalar double to unsigned DWORD/QWORD
-//INST3(vcvtsd2usi,        "cvtsd2usi",       IUM_WR, BAD_CODE,     BAD_CODE,     SSEDBL(0x79),                            INS_TT_TUPLE1_FIXED,                 Input_64Bit    | REX_W1_EVEX                  | Encoding_EVEX)    
 INST3(vcvtss2usi,       "cvtss2usi",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEFLT(0x79),                  INS_TT_TUPLE1_FIXED,                 Input_32Bit    | REX_WX                       | Encoding_EVEX)                                                                                                                                  // cvt scalar single to unsigned DWORD/QWORD
 INST3(vcvttpd2udq,      "cvttpd2udq",       IUM_WR, BAD_CODE,               BAD_CODE,     PCKFLT(0x78),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt w/ truncation packed doubles to unsigned DWORDs
 INST3(vcvttps2udq,      "cvttps2udq",       IUM_WR, BAD_CODE,               BAD_CODE,     PCKFLT(0x78),                  INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // cvt w/ truncation packed singles to unsigned DWORDs
-//INST3(vcvttps2udq,        "cvttps2udq",     IUM_WR, BAD_CODE,     BAD_CODE,     PCKFLT(0x78),                            INS_TT_FULL,                         Input_32Bit    | REX_W0_EVEX                  | Encoding_EVEX)    // cvt packed quad word to double
 INST3(vcvttsd2usi,      "cvttsd2usi",       IUM_WR, BAD_CODE,               BAD_CODE,     SSEDBL(0x78),                  INS_TT_TUPLE1_FIXED,                 Input_64Bit    | REX_WX                       | Encoding_EVEX)                                                                                                                                  // cvt w/ truncation scalar double to unsigned DWORD/QWORD
 INST3(vcvttss2usi32,    "cvttss2usi",       IUM_WR, BAD_CODE,               BAD_CODE,     SSEFLT(0x78),                  INS_TT_TUPLE1_FIXED,                 Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // cvt w/ truncation scalar single to unsigned DWORD/QWORD
 INST3(vcvttss2usi64,    "cvttss2usi",       IUM_WR, BAD_CODE,               BAD_CODE,     SSEFLT(0x78),                  INS_TT_TUPLE1_FIXED,                 Input_32Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt w/ truncation scalar single to unsigned DWORD/QWORD
 INST3(vcvtudq2pd,       "cvtudq2pd",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEFLT(0x7A),                  INS_TT_HALF,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // cvt packed unsigned DWORDs to doubles
 INST3(vcvtudq2ps,       "cvtudq2ps",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEDBL(0x7A),                  INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // cvt packed unsigned DWORDs to singles
-//INST3(vcvtudq2ps,         "cvtudq2ps",      IUM_WR, BAD_CODE,     BAD_CODE,     SSEDBL(0x7A),                            INS_TT_FULL,                         Input_32Bit    | REX_W0_EVEX                  | Encoding_EVEX)    // cvt packed unsigned quad word to double
 INST3(vcvtusi2sd32,     "cvtusi2sd",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEDBL(0x7B),                  INS_TT_TUPLE1_SCALAR,                Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // cvt scalar unsigned DWORD to double
 INST3(vcvtusi2sd64,     "cvtusi2sd",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEDBL(0x7B),                  INS_TT_TUPLE1_SCALAR,                Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // cvt scalar unsigned QWORD to double
 INST3(vcvtusi2ss32,     "cvtusi2ss",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEFLT(0x7B),                  INS_TT_TUPLE1_SCALAR,                Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // cvt scalar unsigned DWORD to single
 INST3(vcvtusi2ss64,     "cvtusi2ss",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEFLT(0x7B),                  INS_TT_TUPLE1_SCALAR,                Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // cvt scalar unsigned QWORD to single
 INST3(vextractf64x4,    "extractf64x4",     IUM_WR, SSE3A(0x1B),            BAD_CODE,     BAD_CODE,                      INS_TT_TUPLE4,                       Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Extract 256-bit packed double-precision floating point values
 INST3(vextracti64x4,    "extracti64x4",     IUM_WR, SSE3A(0x3B),            BAD_CODE,     BAD_CODE,                      INS_TT_TUPLE4,                       Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Extract 256-bit packed quadword integer values
 INST3(vfixupimmpd,      "fixupimmpd",       IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x54),                   INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Fixup special packed double-precision floating-point values
 INST3(vfixupimmps,      "fixupimmps",       IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x54),                   INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Fixup special packed single-precision floating-point values
 INST3(vfixupimmsd,      "fixupimmsd",       IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x55),                   INS_TT_TUPLE1_SCALAR,                Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Fixup special scalar double-precision floating-point value
 INST3(vfixupimmss,      "fixupimmss",       IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x55),                   INS_TT_TUPLE1_SCALAR,                Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Fixup special scalar single-precision floating-point value
 INST3(vgetexppd,        "getexppd",         IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x42),                   INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Extract exponents of packed double-precision floating-point values
 INST3(vgetexpps,        "getexpps",         IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x42),                   INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // Extract exponents of packed single-precision floating-point values
 INST3(vgetexpsd,        "getexpsd",         IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x43),                   INS_TT_TUPLE1_SCALAR,                Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstSrcSrcAVXInstruction)                                                                                           // Extract exponents of scalar double-precision floating-point value
 INST3(vgetexpss,        "getexpss",         IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x43),                   INS_TT_TUPLE1_SCALAR,                Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstSrcSrcAVXInstruction)                                                                                           // Extract exponents of scalar single-precision floating-point value
 INST3(vgetmantpd,       "getmantpd",        IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x26),                   INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Extract mantissas of packed double-precision floating-point values
 INST3(vgetmantps,       "getmantps",        IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x26),                   INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // Extract mantissas of packed single-precision floating-point values
 INST3(vgetmantsd,       "getmantsd",        IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x27),                   INS_TT_TUPLE1_SCALAR,                Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstSrcSrcAVXInstruction)                                                                                           // Extract mantissas of scalar double-precision floating-point value
 INST3(vgetmantss,       "getmantss",        IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x27),                   INS_TT_TUPLE1_SCALAR,                Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstSrcSrcAVXInstruction)                                                                                           // Extract mantissas of scalar single-precision floating-point value
 INST3(vinsertf64x4,     "insertf64x4",      IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x1A),                   INS_TT_TUPLE4,                       Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Insert 256-bit packed double-precision floating point values
 INST3(vinserti64x4,     "inserti64x4",      IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x3A),                   INS_TT_TUPLE4,                       Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Insert 256-bit packed quadword integer values
@@ -817,50 +814,46 @@ INST3(kandnb,           "kandnb",           IUM_WR, BAD_CODE,               BAD_
 INST3(kmovb_gpr,        "kmovb",            IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x92),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Move from and to mask registers
 INST3(kmovb_msk,        "kmovb",            IUM_WR, PCKDBL(0x91),           BAD_CODE,     PCKDBL(0x90),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Move from and to mask registers
 INST3(knotb,            "knotb",            IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x44),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // NOT mask register
 INST3(korb,             "korb",             IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x45),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Bitwise logical OR masks
 INST3(kortestb,         "kortestb",         IUM_RD, BAD_CODE,               BAD_CODE,     PCKDBL(0x98),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX | Resets_OF      | Resets_SF     | Writes_ZF     | Resets_AF     | Resets_PF     | Writes_CF      | KInstruction)                  // OR masks and set flags
 INST3(kshiftlb,         "kshiftlb",         IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x32),                   INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Shift left mask registers
 INST3(kshiftrb,         "kshiftrb",         IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x30),                   INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Shift right mask registers
 INST3(ktestb,           "ktestb",           IUM_RD, BAD_CODE,               BAD_CODE,     PCKDBL(0x99),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX | Resets_OF      | Resets_SF     | Writes_ZF     | Resets_AF     | Resets_PF     | Writes_CF      | KInstruction)                  // Packed bit test masks and set flags
 INST3(ktestw,           "ktestw",           IUM_RD, BAD_CODE,               BAD_CODE,     PCKFLT(0x99),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX | Resets_OF      | Resets_SF     | Writes_ZF     | Resets_AF     | Resets_PF     | Writes_CF      | KInstruction)                  // Packed bit test masks and set flags
 INST3(kxnorb,           "kxnorb",           IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x46),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Bitwise logical XNOR masks
 INST3(kxorb,            "kxorb",            IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x47),                  INS_TT_NONE,                                          REX_W0                       | Encoding_VEX                                                                                                   | KInstruction)                  // Bitwise logical XOR masks
 INST3(vbroadcastf32x2,  "broadcastf32x2",   IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x19),                   INS_TT_TUPLE2,                       Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // Broadcast packed float values read from memory to entire register
 INST3(vbroadcasti32x2,  "broadcasti32x2",   IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x59),                   INS_TT_TUPLE2,                       Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // Broadcast packed integer values read from memory to entire register
 INST3(vbroadcastf32x8,  "broadcastf32x8",   IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x1B),                   INS_TT_TUPLE8,                       Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // Broadcast packed float values read from memory to entire register
 INST3(vbroadcasti32x8,  "broadcasti32x8",   IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x5B),                   INS_TT_TUPLE8,                       Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // Broadcast packed integer values read from memory to entire register
 INST3(vcvtpd2qq,        "cvtpd2qq",         IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x7B),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt packed doubles to   signed QWORDs
 INST3(vcvtpd2uqq,       "cvtpd2uqq",        IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x79),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt packed doubles to unsigned QWORDs
 INST3(vcvtps2qq,        "cvtps2qq",         IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x7B),                  INS_TT_HALF,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // cvt packed singles to   signed QWORDs
 INST3(vcvtps2uqq,       "cvtps2uqq",        IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x79),                  INS_TT_HALF,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // cvt packed singles to unsigned QWORDs
 INST3(vcvtqq2pd,        "cvtqq2pd",         IUM_WR, BAD_CODE,               BAD_CODE,     SSEFLT(0xE6),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt packed signed QWORDs to doubles
-//INST3(vcvtqq2pd,          "cvtqq2pd",       IUM_WR, BAD_CODE,     BAD_CODE,     SSEFLT(0xE6),                            INS_TT_FULL,                         Input_64Bit    | REX_W1_EVEX                  | Encoding_EVEX)    // cvt packed quad word to double
 INST3(vcvtqq2ps,        "cvtqq2ps",         IUM_WR, BAD_CODE,               BAD_CODE,     PCKFLT(0x5B),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt packed signed QWORDs to singles
 INST3(vcvttpd2qq,       "cvttpd2qq",        IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x7A),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt w/ truncation packed doubles to   signed QWORDs
-//INST3(vcvttpd2qq,         "cvttpd2qq",      IUM_WR, BAD_CODE,     BAD_CODE,     PCKDBL(0x7A),                            INS_TT_FULL,                         Input_64Bit    | REX_W1_EVEX                  | Encoding_EVEX)    // cvt packed quad word to double
 INST3(vcvttpd2uqq,      "cvttpd2uqq",       IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x78),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt w/ truncation packed doubles to unsigned QWORDs
-//INST3(vcvttpd2uqq,        "cvttpd2uqq",     IUM_WR, BAD_CODE,     BAD_CODE,     PCKDBL(0x78),                            INS_TT_FULL,                         Input_64Bit    | REX_W1_EVEX                  | Encoding_EVEX)   // cvt packed quad word to double
 INST3(vcvttps2qq,       "cvttps2qq",        IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x7A),                  INS_TT_HALF,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // cvt w/ truncation packed singles to   signed QWORDs
 INST3(vcvttps2uqq,      "cvttps2uqq",       IUM_WR, BAD_CODE,               BAD_CODE,     PCKDBL(0x78),                  INS_TT_HALF,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // cvt w/ truncation packed singles to unsigned QWORDs
 INST3(vcvtuqq2pd,       "cvtuqq2pd",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEFLT(0x7A),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt packed signed QWORDs to doubles
-//INST3(vcvtuqq2pd,         "cvtuqq2pd",      IUM_WR, BAD_CODE,     BAD_CODE,     SSEFLT(0x7A),                            INS_TT_FULL,                         Input_64Bit    | REX_W1_EVEX                  | Encoding_EVEX)    // cvt packed unsigned quad word to double
 INST3(vcvtuqq2ps,       "cvtuqq2ps",        IUM_WR, BAD_CODE,               BAD_CODE,     SSEDBL(0x7A),                  INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // cvt packed signed QWORDs to singles
 INST3(vextractf32x8,    "extractf32x8",     IUM_WR, SSE3A(0x1B),            BAD_CODE,     BAD_CODE,                      INS_TT_TUPLE8,                       Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // Extract 256-bit packed double-precision floating point values
 INST3(vextractf64x2,    "extractf64x2",     IUM_WR, SSE3A(0x19),            BAD_CODE,     BAD_CODE,                      INS_TT_TUPLE2,                       Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Extract 256-bit packed double-precision floating point values
 INST3(vextracti32x8,    "extracti32x8",     IUM_WR, SSE3A(0x3B),            BAD_CODE,     BAD_CODE,                      INS_TT_TUPLE8,                       Input_32Bit    | REX_W0                       | Encoding_EVEX)                                                                                                                                  // Extract 256-bit packed quadword integer values
 INST3(vextracti64x2,    "extracti64x2",     IUM_WR, SSE3A(0x39),            BAD_CODE,     BAD_CODE,                      INS_TT_TUPLE2,                       Input_64Bit    | REX_W1                       | Encoding_EVEX)                                                                                                                                  // Extract 256-bit packed quadword integer values
 INST3(vinsertf32x8,     "insertf32x8",      IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x1A),                   INS_TT_TUPLE8,                       Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Insert 256-bit packed double-precision floating point values
 INST3(vinsertf64x2,     "insertf64x2",      IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x18),                   INS_TT_TUPLE2,                       Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Insert 256-bit packed double-precision floating point values
 INST3(vinserti32x8,     "inserti32x8",      IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x3A),                   INS_TT_TUPLE8,                       Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Insert 256-bit packed quadword integer values
 INST3(vinserti64x2,     "inserti64x2",      IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x38),                   INS_TT_TUPLE2,                       Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Insert 256-bit packed quadword integer values
 INST3(vpcmpd,           "pcmpd",            IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x1F),                   INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_Is3OperandInstructionMask)
 INST3(vpcmpq,           "pcmpq",            IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x1F),                   INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_Is3OperandInstructionMask)
 INST3(vpcmpud,          "pcmpud",           IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x1E),                   INS_TT_FULL,                         Input_32Bit    | REX_W0                       | Encoding_EVEX  | INS_Flags_Is3OperandInstructionMask)
 INST3(vpcmpuq,          "pcmpuq",           IUM_WR, BAD_CODE,               BAD_CODE,     SSE3A(0x1E),                   INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_Is3OperandInstructionMask)
 INST3(vpmovd2m,         "pmovd2m",          IUM_WR, BAD_CODE,               BAD_CODE,     PSSE38(0xF3, 0x39),            INS_TT_NONE,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)
 INST3(vpmovm2d,         "pmovm2d",          IUM_WR, BAD_CODE,               BAD_CODE,     PSSE38(0xF3, 0x38),            INS_TT_NONE,                         Input_32Bit    | REX_W0                       | Encoding_EVEX)
 INST3(vpmovm2q,         "pmovm2q",          IUM_WR, BAD_CODE,               BAD_CODE,     PSSE38(0xF3, 0x38),            INS_TT_NONE,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)
 INST3(vpmovq2m,         "pmovq2m",          IUM_WR, BAD_CODE,               BAD_CODE,     PSSE38(0xF3, 0x39),            INS_TT_NONE,                         Input_64Bit    | REX_W1                       | Encoding_EVEX)
 INST3(vpmullq,          "pmullq",           IUM_WR, BAD_CODE,               BAD_CODE,     SSE38(0x40),                   INS_TT_FULL,                         Input_64Bit    | REX_W1                       | Encoding_EVEX  | INS_Flags_IsDstDstSrcAVXInstruction)                                                                                           // Packed multiply 64 bit unsigned integers and store lower 64 bits of each result
 
 // AVX512VBMI
diff --git a/src/coreclr/jit/morph.cpp b/src/coreclr/jit/morph.cpp
index d9228190c5f..7d9758f0cb9 100644
--- a/src/coreclr/jit/morph.cpp
+++ b/src/coreclr/jit/morph.cpp
@@ -275,71 +275,40 @@ GenTree* Compiler::fgMorphIntoHelperCall(GenTree* tree, int helper, bool morphAr
 //     CAST(int <- MUL(long, long)) => MUL(CAST(int <- long), CAST(int <- long)).
 //     The purpose of this is to allow "optNarrowTree" in the post-order
 //     traversal to fold the tree into a TYP_INT one, which helps 32 bit
 //     targets (and AMD64 too since 32 bit instructions are more compact).
 //     TODO-Arm64-CQ: Re-evaluate the value of this optimization for ARM64.
 //
 // Arguments:
 //    tree - the cast tree to morph
 //
 // Return Value:
 //    The fully morphed tree, or "nullptr" if it needs further morphing,
 //    in which case the cast may be transformed into an unchecked one
 //    and its operand changed (the cast "expanded" into two).
 //
 GenTree* Compiler::fgMorphExpandCast(GenTreeCast* tree)
 {
     GenTree*  oper    = tree->CastOp();
     var_types srcType = genActualType(oper);
     var_types dstType = tree->CastToType();
     unsigned  dstSize = genTypeSize(dstType);
-/*
-    // See if the cast can be contracted into a single optimized cast
-#if defined(TARGET_AMD64)
-    if (compOpportunisticallyDependsOn(InstructionSet_AVX512F))
-    {
-        if (oper->OperIs(GT_CAST))
-        {
-            GenTreeCast *innerCast = static_cast<GenTreeCast*>(oper);
-            GenTree* innerOper = innerCast->CastOp();
-            var_types innerSrcType = genActualType(innerOper);
-            var_types innerDstType = innerCast->CastToType();
-            unsigned  innerDstSize = genTypeSize(innerDstType);
-
-            if (innerCast->IsUnsigned())
-            {
-                innerSrcType = varTypeToUnsigned(innerSrcType);
-
-                if (innerSrcType == TYP_UINT)
-                {
-                    if (dstType == TYP_FLOAT && innerDstType == TYP_DOUBLE)
-                    {
-                        // One optimized cast here
-                        tree = gtNewCastNode(TYP_UINT, innerOper, true, TYP_FLOAT);
-                        return fgMorphTree(tree);
-                    }
-                }
-            }
-        }
-    }
-#endif
-*/
 
 #if defined(TARGET_AMD64)
     // If AVX512 is present, we have intrinsic available to convert
     // ulong directly to float. Hence, we need to combine the 2 nodes
     // GT_CAST(GT_CAST(TYP_ULONG, TYP_DOUBLE), TYP_FLOAT) into a single
     // node i.e. GT_CAST(TYP_ULONG, TYP_FLOAT). At this point, we already
     // have the 2 GT_CAST nodes in the tree and we are combining them below.
     if (compOpportunisticallyDependsOn(InstructionSet_AVX512F))
     {
         if (oper->OperIs(GT_CAST))
         {
             GenTreeCast* innerCast = static_cast<GenTreeCast*>(oper);
 
             if (innerCast->IsUnsigned())
             {
                 GenTree*  innerOper    = innerCast->CastOp();
                 var_types innerSrcType = genActualType(innerOper);
                 var_types innerDstType = innerCast->CastToType();
                 unsigned  innerDstSize = genTypeSize(innerDstType);
                 innerSrcType           = varTypeToUnsigned(innerSrcType);
@@ -524,44 +493,44 @@ GenTree* Compiler::fgMorphExpandCast(GenTreeCast* tree)
             if (dstType == TYP_FLOAT)
             {
                 // Codegen can handle U8 -> R8 conversion.
                 // U8 -> R4 =  U8 -> R8 -> R4
                 // - change the dsttype to double
                 // - insert a cast from double to float
                 // - recurse into the resulting tree
                 tree->CastToType() = TYP_DOUBLE;
                 tree->gtType       = TYP_DOUBLE;
                 tree               = gtNewCastNode(TYP_FLOAT, tree, false, TYP_FLOAT);
 
                 return fgMorphTree(tree);
             }
         }
         else if (srcType == TYP_UINT)
         {
 #if defined(TARGET_AMD64)
             if (!compOpportunisticallyDependsOn(InstructionSet_AVX512F))
             {
 #endif
-            oper = gtNewCastNode(TYP_LONG, oper, true, TYP_LONG);
-            oper->gtFlags |= (tree->gtFlags & (GTF_OVERFLOW | GTF_EXCEPT));
-            tree->ClearUnsigned();
-            tree->CastOp() = oper;
+                oper = gtNewCastNode(TYP_LONG, oper, true, TYP_LONG);
+                oper->gtFlags |= (tree->gtFlags & (GTF_OVERFLOW | GTF_EXCEPT));
+                tree->ClearUnsigned();
+                tree->CastOp() = oper;
 #if defined(TARGET_AMD64)
             }
 #endif
         }
     }
 #endif // TARGET_AMD64
 
 #ifdef TARGET_X86
     // Do we have to do two step U4/8 -> R4/8 ?
     else if (tree->IsUnsigned() && varTypeIsFloating(dstType))
     {
         srcType = varTypeToUnsigned(srcType);
 
         if (srcType == TYP_ULONG)
         {
             return fgMorphCastIntoHelper(tree, CORINFO_HELP_ULNG2DBL, oper);
         }
         else if (srcType == TYP_UINT)
         {
             oper = gtNewCastNode(TYP_LONG, oper, true, TYP_LONG);
diff --git a/src/coreclr/jit/simdashwintrinsic.cpp b/src/coreclr/jit/simdashwintrinsic.cpp
index 268f5d45276..46802bdf858 100644
--- a/src/coreclr/jit/simdashwintrinsic.cpp
+++ b/src/coreclr/jit/simdashwintrinsic.cpp
@@ -1219,97 +1219,97 @@ GenTree* Compiler::impSimdAsHWIntrinsicSpecial(NamedIntrinsic       intrinsic,
                 {
                     return gtNewSimdWidenUpperNode(retType, op1, simdBaseJitType, simdSize);
                 }
 
 #if defined(TARGET_XARCH)
                 case NI_VectorT128_ConvertToInt32:
                 case NI_VectorT256_ConvertToInt32:
                 {
                     assert(simdBaseType == TYP_FLOAT);
                     NamedIntrinsic convert = (simdSize == 32) ? NI_AVX_ConvertToVector256Int32WithTruncation
                                                               : NI_SSE2_ConvertToVector128Int32WithTruncation;
                     return gtNewSimdHWIntrinsicNode(retType, op1, convert, simdBaseJitType, simdSize);
                 }
 
                 case NI_VectorT128_ConvertToDouble:
                 case NI_VectorT256_ConvertToDouble:
                 {
                     assert(sig->numArgs == 1);
                     assert(simdBaseType == TYP_LONG || simdBaseType == TYP_ULONG);
                     NamedIntrinsic convert = (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256Double
-                                                : NI_AVX512DQ_VL_ConvertToVector128Double;
+                                                              : NI_AVX512DQ_VL_ConvertToVector128Double;
                     return gtNewSimdHWIntrinsicNode(retType, op1, convert, simdBaseJitType, simdSize);
                 }
 
                 case NI_VectorT128_ConvertToInt64:
                 case NI_VectorT256_ConvertToInt64:
                 {
                     assert(sig->numArgs == 1);
                     assert(simdBaseType == TYP_DOUBLE);
 #ifdef TARGET_XARCH
                     NamedIntrinsic convert = (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256Int64WithTruncation
-                                                : NI_AVX512DQ_VL_ConvertToVector128Int64WithTruncation;
+                                                              : NI_AVX512DQ_VL_ConvertToVector128Int64WithTruncation;
 #else
                     NamedIntrinsic convert = (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256Int64
-                                                : NI_AVX512DQ_VL_ConvertToVector128Int64;
+                                                              : NI_AVX512DQ_VL_ConvertToVector128Int64;
 #endif // TARGET_XARCH
                     return gtNewSimdHWIntrinsicNode(retType, op1, convert, simdBaseJitType, simdSize);
                 }
 
                 case NI_VectorT128_ConvertToUInt64:
                 case NI_VectorT256_ConvertToUInt64:
                 {
                     assert(sig->numArgs == 1);
                     assert(simdBaseType == TYP_DOUBLE);
 #ifdef TARGET_XARCH
                     NamedIntrinsic convert = (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256UInt64WithTruncation
-                                                : NI_AVX512DQ_VL_ConvertToVector128UInt64WithTruncation;
+                                                              : NI_AVX512DQ_VL_ConvertToVector128UInt64WithTruncation;
 #else
                     NamedIntrinsic convert = (simdSize == 32) ? NI_AVX512DQ_VL_ConvertToVector256UInt64
-                                                : NI_AVX512DQ_VL_ConvertToVector128UInt64;
-#endif //TARGET_XARCH
+                                                              : NI_AVX512DQ_VL_ConvertToVector128UInt64;
+#endif // TARGET_XARCH
                     return gtNewSimdHWIntrinsicNode(retType, op1, convert, simdBaseJitType, simdSize);
                 }
 
                 case NI_VectorT128_ConvertToUInt32:
                 case NI_VectorT256_ConvertToUInt32:
                 {
                     assert(sig->numArgs == 1);
                     assert(simdBaseType == TYP_FLOAT);
                     NamedIntrinsic convert = (simdSize == 32) ? NI_AVX512F_VL_ConvertToVector256UInt32
-                                                : NI_AVX512F_VL_ConvertToVector128UInt32;
+                                                              : NI_AVX512F_VL_ConvertToVector128UInt32;
                     return gtNewSimdHWIntrinsicNode(retType, op1, convert, simdBaseJitType, simdSize);
                 }
 
                 case NI_VectorT128_ConvertToSingle:
                 case NI_VectorT256_ConvertToSingle:
                 {
                     if (simdBaseType == TYP_INT)
                     {
                         NamedIntrinsic convert =
                             (simdSize == 32) ? NI_AVX_ConvertToVector256Single : NI_SSE2_ConvertToVector128Single;
                         return gtNewSimdHWIntrinsicNode(retType, op1, convert, simdBaseJitType, simdSize);
                     }
                     else if (simdBaseType == TYP_UINT)
                     {
-                        NamedIntrinsic convert =
-                            (simdSize == 32) ? NI_AVX512F_VL_ConvertToVector256Single : NI_AVX512F_VL_ConvertToVector128Single;
+                        NamedIntrinsic convert = (simdSize == 32) ? NI_AVX512F_VL_ConvertToVector256Single
+                                                                  : NI_AVX512F_VL_ConvertToVector128Single;
                         return gtNewSimdHWIntrinsicNode(retType, op1, convert, simdBaseJitType, simdSize);
                     }
                     else
                     {
                         unreached();
                     }
                 }
 
                 case NI_VectorT256_ToScalar:
                 {
 #if defined(TARGET_X86)
                     if (varTypeIsLong(simdBaseType))
                     {
                         op2 = gtNewIconNode(0);
                         return gtNewSimdGetElementNode(retType, op1, op2, simdBaseJitType, simdSize);
                     }
 #endif // TARGET_X86
 
                     return gtNewSimdHWIntrinsicNode(retType, op1, NI_Vector256_ToScalar, simdBaseJitType, simdSize);
                 }
